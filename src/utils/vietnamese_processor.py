import os
import emoji
import urllib
import requests
import regex as re
from langdetect import detect

from io import StringIO
from vncorenlp import VnCoreNLP
from transformers import pipeline
import json
import pandas as pd
import glob
from pathlib import Path
import unicodedata
from underthesea import word_tokenize


# https://ihateregex.io
class VietnameseTextCleaner:
    VN_CHARS = '√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê'
    
    @staticmethod
    def remove_html(text):
        """
        X√≥a c√°c th·∫ª HTML kh·ªèi vƒÉn b·∫£n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a c√°c th·∫ª HTML
        Returns:
            str: VƒÉn b·∫£n ƒë√£ x√≥a c√°c th·∫ª HTML
        """
        return re.sub(r'<[^>]*>', '', text)
    
    @staticmethod
    def remove_emoji(text):
        """
        X√≥a c√°c k√Ω t·ª± emoji kh·ªèi vƒÉn b·∫£n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a emoji
        Returns:
            str: VƒÉn b·∫£n ƒë√£ x√≥a emoji
        """
        return emoji.replace_emoji(text, '')
    
    @staticmethod
    def remove_url(text):
        """
        X√≥a c√°c URL kh·ªèi vƒÉn b·∫£n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a URL
        Returns:
            str: VƒÉn b·∫£n ƒë√£ x√≥a URL
        """
        return re.sub(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)', '', text)
    
    @staticmethod
    def remove_email(text):
        """
        X√≥a ƒë·ªãa ch·ªâ email kh·ªèi vƒÉn b·∫£n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a email
        Returns:
            str: VƒÉn b·∫£n ƒë√£ x√≥a email
        """
        return re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
    
    @staticmethod
    def remove_phone_number(text):
        """
        X√≥a s·ªë ƒëi·ªán tho·∫°i kh·ªèi vƒÉn b·∫£n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a s·ªë ƒëi·ªán tho·∫°i
        Returns:
            str: VƒÉn b·∫£n ƒë√£ x√≥a s·ªë ƒëi·ªán tho·∫°i
        """
        return re.sub(r'^[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}$', '', text)
    
    @staticmethod
    def remove_hashtags(text):
        """
        X√≥a hashtag kh·ªèi vƒÉn b·∫£n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a hashtag
        Returns:
            str: VƒÉn b·∫£n ƒë√£ x√≥a hashtag
        """
        return re.sub(r'#\w+', '', text)
    
    @staticmethod
    def remove_unnecessary_characters(text):
        """
        X√≥a c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt v√† chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn l√†m s·∫°ch
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch v√† chu·∫©n h√≥a kho·∫£ng tr·∫Øng
        """
        text = re.sub(fr"[^\sa-zA-Z0-9{VietnameseTextCleaner.VN_CHARS}]", ' ', text)
        return re.sub(r'\s+', ' ', text).strip() # Remove extra whitespace
    
    @staticmethod
    def process_text(text):
        """
        X·ª≠ l√Ω vƒÉn b·∫£n b·∫±ng c√°ch √°p d·ª•ng t·∫•t c·∫£ c√°c ph∆∞∆°ng th·ª©c l√†m s·∫°ch
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn x·ª≠ l√Ω
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω v√† l√†m s·∫°ch
        """
        text = VietnameseTextCleaner.remove_html(text)
        text = VietnameseTextCleaner.remove_emoji(text)
        text = VietnameseTextCleaner.remove_url(text)
        text = VietnameseTextCleaner.remove_email(text)
        text = VietnameseTextCleaner.remove_phone_number(text)
        text = VietnameseTextCleaner.remove_hashtags(text)
        return VietnameseTextCleaner.remove_unnecessary_characters(text)

    @staticmethod
    def is_english_comment(text):
        """
        Ki·ªÉm tra xem comment c√≥ ph·∫£i l√† ti·∫øng Anh kh√¥ng
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn ki·ªÉm tra
        Returns:
            bool: True n·∫øu l√† ti·∫øng Anh, False n·∫øu kh√¥ng ph·∫£i
        """
        try:
            return detect(text) == 'en'
        except:
            return False

class VietnameseToneNormalizer:
    VOWELS_TABLE = [
        ['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a'],
        ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],
        ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],
        ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e' ],
        ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],
        ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i' ],
        ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o' ],
        ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],
        ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],
        ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u' ],
        ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],
        ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y']
    ]
    
    VOWELS_TO_IDS = {
        'a': (0, 0), '√†': (0, 1), '√°': (0, 2), '·∫£': (0, 3), '√£': (0, 4), '·∫°': (0, 5), 
        'ƒÉ': (1, 0), '·∫±': (1, 1), '·∫Ø': (1, 2), '·∫≥': (1, 3), '·∫µ': (1, 4), '·∫∑': (1, 5), 
        '√¢': (2, 0), '·∫ß': (2, 1), '·∫•': (2, 2), '·∫©': (2, 3), '·∫´': (2, 4), '·∫≠': (2, 5), 
        'e': (3, 0), '√®': (3, 1), '√©': (3, 2), '·∫ª': (3, 3), '·∫Ω': (3, 4), '·∫π': (3, 5), 
        '√™': (4, 0), '·ªÅ': (4, 1), '·∫ø': (4, 2), '·ªÉ': (4, 3), '·ªÖ': (4, 4), '·ªá': (4, 5), 
        'i': (5, 0), '√¨': (5, 1), '√≠': (5, 2), '·ªâ': (5, 3), 'ƒ©': (5, 4), '·ªã': (5, 5), 
        'o': (6, 0), '√≤': (6, 1), '√≥': (6, 2), '·ªè': (6, 3), '√µ': (6, 4), '·ªç': (6, 5), 
        '√¥': (7, 0), '·ªì': (7, 1), '·ªë': (7, 2), '·ªï': (7, 3), '·ªó': (7, 4), '·ªô': (7, 5), 
        '∆°': (8, 0), '·ªù': (8, 1), '·ªõ': (8, 2), '·ªü': (8, 3), '·ª°': (8, 4), '·ª£': (8, 5), 
        'u': (9, 0), '√π': (9, 1), '√∫': (9, 2), '·ªß': (9, 3), '≈©': (9, 4), '·ª•': (9, 5), 
        '∆∞': (10, 0), '·ª´': (10, 1), '·ª©': (10, 2), '·ª≠': (10, 3), '·ªØ': (10, 4), '·ª±': (10, 5), 
        'y': (11, 0), '·ª≥': (11, 1), '√Ω': (11, 2), '·ª∑': (11, 3), '·ªπ': (11, 4), '·ªµ': (11, 5)
    }
    
    VINAI_NORMALIZED_TONE = {
        '√≤a': 'o√†', '√ía': 'O√†', '√íA': 'O√Ä', 
        '√≥a': 'o√°', '√ìa': 'O√°', '√ìA': 'O√Å', 
        '·ªèa': 'o·∫£', '·ªéa': 'O·∫£', '·ªéA': 'O·∫¢',
        '√µa': 'o√£', '√ïa': 'O√£', '√ïA': 'O√É',
        '·ªça': 'o·∫°', '·ªåa': 'O·∫°', '·ªåA': 'O·∫†',
        '√≤e': 'o√®', '√íe': 'O√®', '√íE': 'O√à',
        '√≥e': 'o√©', '√ìe': 'O√©', '√ìE': 'O√â',
        '·ªèe': 'o·∫ª', '·ªée': 'O·∫ª', '·ªéE': 'O·∫∫',
        '√µe': 'o·∫Ω', '√ïe': 'O·∫Ω', '√ïE': 'O·∫º',
        '·ªçe': 'o·∫π', '·ªåe': 'O·∫π', '·ªåE': 'O·∫∏',
        '√πy': 'u·ª≥', '√ôy': 'U·ª≥', '√ôY': 'U·ª≤',
        '√∫y': 'u√Ω', '√öy': 'U√Ω', '√öY': 'U√ù',
        '·ªßy': 'u·ª∑', '·ª¶y': 'U·ª∑', '·ª¶Y': 'U·ª∂',
        '≈©y': 'u·ªπ', '≈®y': 'U·ªπ', '≈®Y': 'U·ª∏',
        '·ª•y': 'u·ªµ', '·ª§y': 'U·ªµ', '·ª§Y': 'U·ª¥',
    }

    @staticmethod
    def normalize_unicode(text):
        """
        Chu·∫©n h√≥a Unicode cho vƒÉn b·∫£n ti·∫øng Vi·ªát
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn chu·∫©n h√≥a
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a Unicode
        """
        char1252 = r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'
        charutf8 = r'√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())
    
    @staticmethod
    # https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
    def normalize_sentence_typing(text, vinai_normalization=False):
        """
        Chu·∫©n h√≥a c√°ch g√µ d·∫•u trong c√¢u ti·∫øng Vi·ªát
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn chu·∫©n h√≥a
            vinai_normalization (bool): C√≥ s·ª≠ d·ª•ng chu·∫©n h√≥a theo VINAI kh√¥ng
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a c√°ch g√µ d·∫•u
        """
        if vinai_normalization:
            for wrong, correct in VietnameseToneNormalizer.VINAI_NORMALIZED_TONE.items():
                text = text.replace(wrong, correct)
            return text.strip()
        
        words = text.strip().split()
        for index, word in enumerate(words):
            cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
            if len(cw) == 3: cw[1] = VietnameseToneNormalizer.normalize_word_typing(cw[1])
            words[index] = ''.join(cw)
        return ' '.join(words)
    
    @staticmethod
    def normalize_word_typing(word):
        """
        Chu·∫©n h√≥a c√°ch g√µ d·∫•u trong t·ª´ ti·∫øng Vi·ªát
        Args:
            word (str): T·ª´ c·∫ßn chu·∫©n h√≥a
        Returns:
            str: T·ª´ ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a c√°ch g√µ d·∫•u
        """
        if not VietnameseToneNormalizer.is_valid_vietnamese_word(word): return word
        chars, vowel_indexes = list(word), []
        qu_or_gi, tonal_mark = False, 0
        
        for index, char in enumerate(chars):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            row, col = VietnameseToneNormalizer.VOWELS_TO_IDS[char]
            if index > 0 and (row, chars[index - 1]) in [(9, 'q'), (5, 'g')]:
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                qu_or_gi = True
                
            if not qu_or_gi or index != 1: vowel_indexes.append(index)
            if col != 0:
                tonal_mark = col
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                
        if len(vowel_indexes) < 2:
            if qu_or_gi:
                index = 1 if len(chars) == 2 else 2
                if chars[index] in VietnameseToneNormalizer.VOWELS_TO_IDS:
                    row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
                    chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                else: chars[1] = VietnameseToneNormalizer.VOWELS_TABLE[5 if chars[1] == 'i' else 9][tonal_mark]
                return ''.join(chars)
            return word
        
        for index in vowel_indexes:
            row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
            if row in [4, 8]: # √™, ∆°
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                return ''.join(chars)
            
        index = vowel_indexes[0 if len(vowel_indexes) == 2 and vowel_indexes[-1] == len(chars) - 1 else 1] 
        row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
        chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
        return ''.join(chars)
    
    @staticmethod
    def is_valid_vietnamese_word(word):
        """
        Ki·ªÉm tra xem m·ªôt t·ª´ c√≥ ph·∫£i l√† t·ª´ ti·∫øng Vi·ªát h·ª£p l·ªá kh√¥ng
        Args:
            word (str): T·ª´ c·∫ßn ki·ªÉm tra
        Returns:
            bool: True n·∫øu l√† t·ª´ ti·∫øng Vi·ªát h·ª£p l·ªá, False n·∫øu kh√¥ng
        """
        vowel_indexes = -1 
        for index, char in enumerate(word):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            if vowel_indexes in [-1, index - 1]: vowel_indexes = index
            else: return False
        return True

class VietnameseTextPreprocessor:
    def __init__(self, vncorenlp_dir='./VnCoreNLP', extra_teencodes=None, max_correction_length=512):
        """
        Kh·ªüi t·∫°o b·ªô ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ti·∫øng Vi·ªát
        Args:
            vncorenlp_dir (str): Th∆∞ m·ª•c ch·ª©a c√°c file VnCoreNLP
            extra_teencodes (dict): C√°c √°nh x·∫° teencode b·ªï sung
            max_correction_length (int): ƒê·ªô d√†i t·ªëi ƒëa cho vi·ªác s·ª≠a l·ªói vƒÉn b·∫£n
        """
        self.vncorenlp_dir = vncorenlp_dir
        self.extra_teencodes = extra_teencodes
        self._load_vncorenlp()
        self._build_teencodes()
        
        self.max_correction_length = max_correction_length
        self.corrector = pipeline(
            'text2text-generation', model='bmd1905/vietnamese-correction-v2', 
            torch_dtype='bfloat16', device_map='auto', num_workers=os.cpu_count()
        )
        print('bmd1905/vietnamese-correction-v2 is loaded successfully.')
        
    
    def _load_vncorenlp(self):
        """
        T·∫£i b·ªô ph√¢n ƒëo·∫°n t·ª´ VnCoreNLP
        """
        self.word_segmenter = None
        if self._get_vncorenlp_files('/VnCoreNLP-1.2.jar') and \
           self._get_vncorenlp_files('/models/wordsegmenter/vi-vocab') and \
           self._get_vncorenlp_files('/models/wordsegmenter/wordsegmenter.rdr'):
            self.word_segmenter = VnCoreNLP(self.vncorenlp_dir + '/VnCoreNLP-1.2.jar', annotators='wseg', quiet=False)
            print('VnCoreNLP word segmenter is loaded successfully.')
        else: print('Failed to load VnCoreNLP word segmenter.')
            

    def _get_vncorenlp_files(self, url_slash):
        """
        T·∫£i c√°c file VnCoreNLP n·∫øu ch√∫ng ch∆∞a t·ªìn t·∫°i
        Args:
            url_slash (str): ƒê∆∞·ªùng d·∫´n URL ƒë·∫øn file
        Returns:
            bool: True n·∫øu file t·ªìn t·∫°i ho·∫∑c t·∫£i th√†nh c√¥ng, False n·∫øu kh√¥ng
        """
        local_path = self.vncorenlp_dir + url_slash
        if os.path.exists(local_path): return True
        
        # Check if the folder contains the local_path exists, if not, create it.
        if not os.path.exists(os.path.dirname(local_path)):
            os.makedirs(os.path.dirname(local_path))
        
        download_url = 'https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master' + url_slash
        try: 
            print(f'Downloading {download_url} to {local_path}')
            return urllib.request.urlretrieve(download_url, local_path)
        except urllib.error.HTTPError as e:
            print(f'Failed to download {download_url} due to {e}')
            return False
                
        
    def _build_teencodes(self):
        """
        X√¢y d·ª±ng t·ª´ ƒëi·ªÉn teencode t·ª´ c√°c √°nh x·∫° m·∫∑c ƒë·ªãnh v√† b·ªï sung
        """
        self.teencodes = {
            'ok': ['okie', 'okey', '√¥k√™', 'oki', 'oke', 'okay', 'ok√™'], 
            'kh√¥ng': ['kg', 'not', 'k', 'kh', 'k√¥', 'hok', 'ko', 'khong'], 'kh√¥ng ph·∫£i': ['kp'], 
            'c·∫£m ∆°n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'h·ªìi ƒë√≥': ['h√πi ƒë√≥'], 'mu·ªën': ['m√∫n'],
            
            'r·∫•t t·ªët': ['perfect', '‚ù§Ô∏è', 'üòç'], 'd·ªÖ th∆∞∆°ng': ['cute'], 'y√™u': ['iu'], 'th√≠ch': ['thik'], 
            't·ªët': [
                'gud', 'good', 'g√∫t', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'
                'üëç', 'üéâ', 'üòÄ', 'üòÇ', 'ü§ó', 'üòô', 'üôÇ'
            ], 
            'b√¨nh th∆∞·ªùng': ['bt', 'bthg'], 'h√†g': ['h√†ng'], 
            'kh√¥ng t·ªët':  ['lol', 'cc', 'huhu', ':(', 'üòî', 'üòì'],
            't·ªá': ['sad', 'por', 'poor', 'bad'], 'gi·∫£ m·∫°o': ['fake'], 
            
            'qu√°': ['wa', 'w√°', 'q√°'], 'ƒë∆∞·ª£c': ['ƒëx', 'dk', 'dc', 'ƒëk', 'ƒëc'], 
            'v·ªõi': ['vs'], 'g√¨': ['j'], 'r·ªìi': ['r'], 'm√¨nh': ['m', 'mik'], 
            'th·ªùi gian': ['time'], 'gi·ªù': ['h'], 
        }
        if self.extra_teencodes: 
            for key, values in self.extra_teencodes.items():
                if any(len(value.split()) > 1 for value in values):
                    raise ValueError('The values for each key in extra_teencodes must be single words.')
                self.teencodes.setdefault(key, []).extend(values)
                
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'
        response = requests.get(teencode_url)
        
        if response.status_code == 200:
            text_data = StringIO(response.text)
            for pair in text_data:
                teencode, true_text = pair.split('\t')
                self.teencodes[teencode.strip()] = true_text.strip()
            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}
        else: print('Failed to fetch teencode.txt from', teencode_url)

    
    def normalize_teencodes(self, text):
        """
        Chu·∫©n h√≥a c√°c teencode trong vƒÉn b·∫£n v·ªÅ d·∫°ng chu·∫©n
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o ch·ª©a teencode
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a teencode
        """
        words = []
        for word in text.split():
            words.append(self.teencodes.get(word, word))
        return ' '.join(words)
    
    # https://huggingface.co/bmd1905/vietnamese-correction-v2
    def correct_vietnamese_errors(self, texts):
        """
        S·ª≠a l·ªói ch√≠nh t·∫£ v√† ng·ªØ ph√°p ti·∫øng Vi·ªát
        Args:
            texts (list): Danh s√°ch c√°c vƒÉn b·∫£n c·∫ßn s·ª≠a
        Returns:
            list: Danh s√°ch c√°c vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c s·ª≠a
        """
        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)
        return [prediction['generated_text'] for prediction in predictions]
        
    
    def word_segment(self, text):
        """
        Th·ª±c hi·ªán ph√¢n ƒëo·∫°n t·ª´ ti·∫øng Vi·ªát
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn ph√¢n ƒëo·∫°n
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c ph√¢n ƒëo·∫°n
        """
        if self.word_segmenter: 
            words = self.word_segmenter.tokenize(text)
            return ' '.join(sum(words, []))
        print('Kh√¥ng c√≥ tr√¨nh ph√¢n ƒëo·∫°n t·ª´ VnCoreNLP n√†o ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ki·ªÉm tra t·ªáp jar VnCoreNLP.')
        return text
        
    
    def process_text(self, text, normalize_tone=True, segment=True):
        """
        X·ª≠ l√Ω vƒÉn b·∫£n v·ªõi c√°c b∆∞·ªõc chu·∫©n h√≥a kh√°c nhau
        Args:
            text (str): VƒÉn b·∫£n ƒë·∫ßu v√†o c·∫ßn x·ª≠ l√Ω
            normalize_tone (bool): C√≥ chu·∫©n h√≥a d·∫•u ti·∫øng Vi·ªát kh√¥ng
            segment (bool): C√≥ th·ª±c hi·ªán ph√¢n ƒëo·∫°n t·ª´ kh√¥ng
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        text = text.lower()
        if normalize_tone:
            text = VietnameseToneNormalizer.normalize_unicode(text)
            text = VietnameseToneNormalizer.normalize_sentence_typing(text)
        text = VietnameseTextCleaner.process_text(text)
        text = self.normalize_teencodes(text)
        return self.word_segment(text) if segment else text
    
    
    def process_batch(self, texts, correct_errors=True):
        """
        X·ª≠ l√Ω m·ªôt lo·∫°t c√°c vƒÉn b·∫£n
        Args:
            texts (list): Danh s√°ch c√°c vƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω
            correct_errors (bool): C√≥ s·ª≠a l·ªói kh√¥ng
        Returns:
            list: Danh s√°ch c√°c vƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        if correct_errors:
            texts = [self.process_text(text, normalize_tone=True, segment=False) for text in texts]
            texts = self.correct_vietnamese_errors(texts)
            return [self.process_text(text, normalize_tone=False, segment=True) for text in texts]
        return [self.process_text(text, normalize_tone=True, segment=True) for text in texts]
    
    
    def close_vncorenlp(self):
        """
        ƒê√≥ng b·ªô ph√¢n ƒëo·∫°n t·ª´ VnCoreNLP
        """
        if self.word_segmenter: 
            print('ƒêang ƒë√≥ng tr√¨nh ph√¢n ƒëo·∫°n t·ª´ VnCoreNLP...')
            self.word_segmenter.close()
   
def adjust_labels(id, original_text, processed_text, original_labels, preprocessor):
    updated_labels = []
    
    # Chu·∫©n h√≥a Unicode g·ªëc v√† ƒë√£ x·ª≠ l√Ω ƒë·ªÉ ƒë·∫£m b·∫£o match theo v·ªã tr√≠ k√Ω t·ª± ƒë√∫ng
    original_text = unicodedata.normalize('NFC', original_text)
    processed_text = unicodedata.normalize('NFC', processed_text)

    for start, end, label in original_labels:
        original_span = original_text[start:end].strip()
        normalized_span = preprocessor.process_text(original_span, normalize_tone=True, segment=True)
        
        search_span = normalized_span.replace('_', ' ')
        search_processed_text = processed_text.replace('_', ' ')

        match = re.search(search_span, search_processed_text, re.IGNORECASE)
        
        if match:
            updated_labels.append([match.start(), match.end(), label])
        else:
            print(f"{id} - Kh√¥ng t√¨m th·∫•y: '{search_span}' trong processed text.")
    return updated_labels

def preprocess_hotel_jsonl(
    input_path: str,
    output_path: str,
    vncorenlp_dir: str = "./VnCoreNLP",
    max_correction_length: int = 512
):
    preprocessor = VietnameseTextPreprocessor(
        vncorenlp_dir=vncorenlp_dir,
        max_correction_length=max_correction_length
    )

    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    with open(input_path, "r", encoding="utf-8", errors="ignore") as f_in, \
         open(output_path, "w", encoding="utf-8", errors="ignore") as f_out:

        for line in f_in:
            if not line.strip():
                continue

            original = json.loads(line)
            original_text = original["data"]
            processed_text = preprocessor.process_text(original_text, normalize_tone=True, segment=True)

            updated_labels = adjust_labels(original.get("id"), original_text, processed_text, original.get("label", []), preprocessor)

            result = {
                "id": original.get("id"),
                "data": processed_text,
                "label": updated_labels,
                "labels": original.get("labels", "")
            }

            f_out.write(json.dumps(result, ensure_ascii=False) + "\n")

    preprocessor.close_vncorenlp()
    print(f"ƒê√£ x·ª≠ l√Ω v√† l∆∞u t·∫°i: {output_path}")

def main():
    print("VietNamese Processor...")

    preprocess_hotel_jsonl(
    input_path="datasets/hotel.jsonl",
    output_path="processed_datasets/hotel.jsonl"
)

if __name__ == '__main__':
    main()